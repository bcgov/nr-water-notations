# Water Notations

## Overview Of Project

Attempting to setup an automated data pipeline that monitors inputs to the
water notations analysis, and when they change triggers a new analysis.

The process will use an openshift namespace to accomplish this. See section
below labeled Openshift

## Data used in Analysis

### Inputs to Water Notations Analysis

The inputs to the analysis are:
* freshwater atlas data
* Water Notations Points

### Outputs of the Analysis

* Water Notations Aquifers
* Water Notations Streams

# Tasks / Worklist

## Freshwater Atlas Data Dump - ~70% complete

A FME Script (FMW) has been created that dumps the freshwater atlas data out
of the BCGW and stores it as a gziped geopackage in object storage.  Simon has
this script.  Work with Simon to get it published to IIT's FME Server and define
a schedule for it.

## Freshwater Atlas Data Load - ~60% complete

The data load is completed by a make file process.  The process is defined in
the fresh water atlas github repo here:

https://github.com/franTarkenton/fwapg

The make file that does the load is [here](https://github.com/franTarkenton/fwapg/blob/main/Makefile)

This [docker file](https://github.com/franTarkenton/fwapg/blob/main/Dockerfile)
includes all the dependencies required to load the freshwater atlas data into the openshift / postgres database.

A github action in the repo https://github.com/franTarkenton/fwapg exists that
creates the container and loads it to dockerhub.  The container is here:

https://hub.docker.com/repository/docker/guylafleur/gdal-util

And the openshift job that does the load is defined in the helm chart here:
https://github.com/bcgov/nr-water-notations/blob/main/cicd/water-notations/templates/dataloadjob.yaml

**Status:**

Have made multiple attempts to get the data loaded.  But haven't managed to get
it load successfully yet.  The logic is there, ideally the process should be set
up so that it can pickup where it last left off.

## Water Notation Points - Change Detection / Load - ~50%

This process is defined in the shell script here:

jobs/waternote.sh

**Status:**
* need to create a openshift cronjob for this process
* logic on simons side is defined.  Outstanding is to get this process to run
reliably in openshift

## DataBC - Aquifers / Streams Injestion - ~80% Complete

Due to the size of the stream data, DataBC does not want to load the data every
night as each load contains almost 2 million records, and can take as long as
5 hours to complete.

Options presented by DataBC included:
* Schedule for weekly replication: Was deemed unacceptable by business area
* Load the data from the Staging Area: Firewall and network policies prevent the
  openshift job from communicating with the staging area.

Proposed Solution:
* IIT will develop a change detection process for object storage.

4-11-2022 - A tentative solution has been delivered to DataBC that contains two
            individual FMW's each with an embedded transformer that does the
            change detection in object storage.

We got the change detection transformer to run correctly on DataBC FME workbench
deployment, but it failed when run on FME Server.  Not sure what the problem was
but have requested console access to IIT Fme Server to try to debug.


# Components to be created

## Openshift

The openshift namespace is made up of the following components:
* postgres/gis database
* *openshift/kubernetes cron Job:* injest freshwater Atlas data when changed
* *openshift/kubernetes cron Job:* download and asses water notations data for
change, and trigger of analysis and output of the Aquifers and Streams if the water notations points have changed

All the components described above are to be defined in the helm chart that
is located in: cicd/water-notations of this repo.

# FME

## FWA Data Dump

Described above in a task.  This is an FMW script that will dump the Freshwater
Atlas data from the BCGW weekly.

## Aquifers / Streams Water Notations - BCGW load

Again identified above.  This process is a FMW run by DataBC that monitors for
Change and loads the data to the BCGW when it has changed.

# Other Misc Outstanding

Currently the database logs are generating the following error:

`2022-04-11 17:17:53.649 UTC [328418] FATAL:  role "1009130000" does not exist`

The message is generated every 10 seconds, which suggests that it is being
generated by the readiness probe which runs every 10 seconds. I can log into the
pod however and run the identical command without any errors.  Suspect that the
shell is being run as an operating system user that does not correspond with a
defined postgres user.  Not sure how to force the shell to run as the `postgres`
operating system user
